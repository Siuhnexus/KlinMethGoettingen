---
title: "Datenanalyse ALM (3) Kontraste"
author: "YH"
date: '2022-05-16'
output:
  word_document: default
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "")
```

### Activate packages
```{r}
library(ggplot2)
library(psych)
library(Hmisc)
library(car)
library(lmtest)
#library(effectsize)
#library(rcompanion)
library(tidyr)
library(predictmeans)
library(ez)
library(lme4)
```
### set working directory

```{r}
setwd("~/Teaching/Master/Methoden_Klippt/Methoden Klippt 2022/Datenbeispiele/4")
```

### Daten einlesen

Auch hier bitte darauf achten, dass das Verzeichnis für die Datei stimmt.
```{r}
dat_1 <- read.csv(file = "~/Teaching/Master/Methoden_Klippt/Methoden Klippt 2022/Datenbeispiele/4/Dat_VR_Anxiety_lm1.csv", header = TRUE, sep = ",", dec = ".", na.strings = "NA")

# Characterstrings als Faktoren definieren
dat_1$Gruppe <- as.factor(dat_1$Gruppe) 
dat_1$Gruppe <- factor(dat_1$Gruppe, levels = c("in_virtuo", "in_sensu", "Warteliste")) 

```
## Transformation der Daten in long Format

wide == Daten jeder Person stehen in einer Zeile
long == Daten der einzelnen Messzeitpunkte stehen in einer Zeile, d.h. jede Person hat mehrere Zeilen

### Warum ist es wichtig Daten ins long Format zu übertragen?

Nur so kann jede der Zeitpunkt der Messung ein Prädiktor sein kann. Jeder Prädiktor muss eine Variable in einer Spalte sein
=> damit kommt jeder Person zwei Mal vor, d.h. in zwei Zeilen

```{r}
dat_1_long <- as.data.frame(pivot_longer(dat_1, cols = c(Pre, Post), names_to = "Zeitpunkt",
values_to = "SP_score", values_drop_na = FALSE))
dat_1_long$Zeitpunkt <- as.factor(dat_1_long$Zeitpunkt)
dat_1_long$Zeitpunkt <- factor(dat_1_long$Zeitpunkt, levels = c("Pre", "Post")) 
dat_1_long$ID <- as.factor(dat_1_long$ID)

```

## Visuelle Analyse

```{r}
# Soziale Phobie als Kriterium/outcome

print(ggplot(dat_1_long, aes(x=Zeitpunkt, y=SP_score)) + 
  geom_violin() + 
  geom_jitter(width = 0.3, height = 0, col = "grey", size = 0.5) +
  facet_wrap(~Gruppe)+
  ylab("Score Soziale Phobie") + 
  xlab("Zeitpunkt") +
  ggtitle("Veränderung Soziale Phobie Prä zu Post"))

print(ggplot(dat_1_long, aes(x=Zeitpunkt, y=SP_score, group = ID)) + 
  facet_wrap(~Gruppe)+
  geom_line(aes(color= ID), show.legend = FALSE) +
  ylab("Score Soziale Phobie") + 
  xlab("Zeitpunkt") +
  ggtitle("Veränderung Soziale Phobie Prä zu Post"))

```


### Deskriptive Analyse

```{r}
descriptives <- describeBy(SP_score ~ Zeitpunkt + Gruppe, mat = TRUE, data = dat_1_long)
descriptives

#### Namen der Faktoren in Ergebnimatrix einfuegen
names(descriptives)[names(descriptives) == "group1"] <- "Zeitpunkt"
names(descriptives)[names(descriptives) == "group2"] <- "Bedingung"
#### Konfidenzintervall ausrechnen
descriptives$ci_95 <- descriptives$se*1.96

descriptives$Zeitpunkt <- factor(descriptives$Zeitpunkt, levels = c("Pre", "Post"))
descriptives$Bedingung <- factor(descriptives$Bedingung, levels = c("in_virtuo", "in_sensu", "Warteliste"))

#### Graphik machen
pd <- position_dodge(0.3) # zum Versetzen der Punkte

ggplot(descriptives, aes(x=Zeitpunkt, y=mean, group = Bedingung)) + 
   geom_errorbar(width=.2, aes(ymin=mean-ci_95, ymax=mean+ci_95), position = pd) +
  geom_point(aes(color = Bedingung), size=2, position = pd) +
    geom_line(aes(linetype=Bedingung, color = Bedingung), position = pd) +
    ggtitle("Mittelwerte plus/minus 95% Konfidenzintervall")+
    theme_classic()
```
# Zu Grunde gelegtes statistisches Modell

Behandlung und Zeitpunkt sowie ihre Interaktion sagen Score in sozialer Phobie vorher

## Analyse Schritt 1: Allgemeines lineares Modell anpassen

ACHTUNG DAS FOLGENDE IST FALSCH UND DIENT NUR DER VERANSCHAULICHUNG

```{r}
# Modelldefinition
contrasts(dat_1_long$Gruppe) <- contr.helmert(3)
M1 <- lm(SP_score ~ Gruppe * Zeitpunkt, data = dat_1_long)

# Schaetzung der Parameter
summary(M1)

# ANOVA zur Bestimmung der Quadratsummen für die beiden Prädiktoren
Anova(M1)
```
### Kritisches Problem des Modells = Abhängigkeit der Messungen
```{r}
# Graphik der Zusammenhänge der Residuen
acf(M1$residuals)

# Test für Autokorrelation 1.Ordnung
durbinWatsonTest(M1) # Test für eine Autokorrelation 1.Ordnung

# Korrelation Prä-Post Messung
cor(dat_1$Pre, dat_1$Post)

```
## Quick fix: Messwiederholungsvarianzanalyse
```{r}
# Option 1: Varianzanalyse
## Definition Kontraste
contrasts(dat_1_long$Gruppe) <- contr.helmert(3)

## Berechnung Varianzanalyse
aov_m2 <- ezANOVA(dat_1_long, dv=SP_score, wid = ID, within = Zeitpunkt, between = Gruppe,  detailed = TRUE, return_aov = TRUE)
aov_m2
```

## Einfachere Lösung; Multi-level Modell
```{r}
# Definition Modell mit random intercept pro Teilnehmer*in = intercept variiert zwischen den Teilnehmer*innen 
M3 <- lmerTest::lmer(SP_score ~ Gruppe * Zeitpunkt + (1|ID), data = dat_1_long)

# Zeigen geschätzte Koeffizienten
summary(M3)

# Signifikanztest für Faktoren
Anova(M3)

# Geschätzte Mittelwerte
predmeans <- predictmeans(M3, "Gruppe:Zeitpunkt", adj ="BH", barplot = TRUE)
predmeans
predmeans$predictmeansBarPlot


```
## Prüfung der Residuen
```{r}
library(nlme)
# Das gleiche Modell nur mit einem anderen Paket gemacht
M4 <- lme(SP_score ~ Gruppe * Zeitpunkt, random = ~1|ID, data = dat_1_long)
summary(M4)

# Residuen
densityplot(residuals(M4, type = "normalized"))
## Normalisierung ist wichtig, damit Abhängigkeiten richtig berücksichtigt werden

# Abspeichern der Residuen und fitted values in Vektoren
res.vec <- residuals(M4, type = "normalized")
fit.vec <- M4$fitted[,1]

# Scatterplot machen
scatterplot(res.vec ~ fit.vec)
```

